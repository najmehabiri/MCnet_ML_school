{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Autoencoder exercises</h1> \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# use interactive backend\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "#!pip install --upgrade tensorflow-probability\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "# Forcing Tenosrflow to use CPU\n",
    "tf.config.experimental.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A function to display images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dis_img(x,y, labeldict, title = None):\n",
    "    \n",
    "    indx = np.random.choice(range(x.shape[0]) , 16 )   \n",
    "    fig, ax = plt.subplots(4, 4,\n",
    "                           figsize=(6,5),\n",
    "                           subplot_kw={'xticks': [], 'yticks': []},\n",
    "                           num= title ) \n",
    "    fig.subplots_adjust(hspace=.4, wspace=0)\n",
    "    imag = [ (x[ind],y[ind])  for ind in indx] \n",
    "    for coef, ax in zip(imag, ax.ravel()):\n",
    "        ax.imshow(coef[0].reshape(28, 28), cmap=plt.cm.gray)     \n",
    "        ax.set_title(labeldict[coef[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and pre-processing the data\n",
    "\n",
    "Original Fashion-MNIST data has $70000$ samples. To make the training process faster, here we have a smaller size of this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "\n",
    "train = np.load('fashion_train.npz')\n",
    "test = np.load('fashion_test.npz')\n",
    "x_train, y_train = train['x'], train['y']\n",
    "x_test, y_test = test['x'], test['y']\n",
    "\n",
    "\n",
    "\n",
    "print('Train-data shape: ', x_train.shape)\n",
    "print('Test-data shape: ', x_test.shape)\n",
    "\n",
    "\n",
    "labeldict = {\n",
    "    0: 'T-shirt/top',\n",
    "    1: 'Trouser',\n",
    "    2: 'Pullover',\n",
    "    3: 'Dress',\n",
    "    4: 'Coat',\n",
    "    5: 'Sandal',\n",
    "    6: 'Shirt',\n",
    "    7: 'Sneaker',\n",
    "    8: 'Bag',\n",
    "    9: 'Ankle boot'\n",
    "}\n",
    "\n",
    "\n",
    "x_train = x_train.reshape(-1, 784).astype('float32') /255.\n",
    "x_test = x_test.reshape(-1, 784).astype('float32') /255.\n",
    "\n",
    "\n",
    "dis_img(x_train,y_train, labeldict,'random_samples')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Autoencoder </h1> \n",
    "\n",
    "\n",
    "#### Two ways of building the models:\n",
    "Encoder $\\rightarrow  z = g_w(x)$\n",
    "\n",
    "Decoder $\\rightarrow  \\tilde{x} = f_w(z)$\n",
    "\n",
    "\n",
    "   * Sequential API with Funstional API:\n",
    "   <br><br>\n",
    "  `encoder = tf.keras.Sequential([ layer_1, ..] )\n",
    "    decoder = tf.keras.Sequential([ layer_1, ..] )\n",
    "    Autoenocder = tf.keras.Model(inputs = enocder.inputs, outputs = decoder(encoder.outputs)`\n",
    " \n",
    "   \n",
    " \n",
    "   \n",
    "   * Model subclassing:\n",
    "   <br><br>\n",
    "   `class Autoencoder(tf.keras.Model):\n",
    "       def __ini__t(self):\n",
    "        super(Autoneocder, self).__init__()\n",
    "        encoder = self.encoder()\n",
    "        decoder = delf.decoder() \n",
    "       def enocder()\n",
    "       def decoder()\n",
    "       `\n",
    "\n",
    "#### Steps:     \n",
    "1. Build encoder, decoder, and the autoencoder as models. \n",
    "2. Decide on the format of the layers. Use a correct activation function that suites the data.\n",
    "3. To build a denoising autoencoder, one can add dropout layers to hidden layers in encoder and decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_dim = 784\n",
    "hidden_dim = 50\n",
    "latent_dim = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder model\n",
    "encoder = tf.keras.Sequential([\n",
    "    tf.keras.Input(inp_dim ),\n",
    "    #tf.keras.layers.Dropout(0.3), # In case we want denoising\n",
    "    tf.keras.layers.Dense(units=hidden_dim , activation='relu'), # hidden layer\n",
    "    tf.keras.layers.Dense(units=latent_dim, activation='relu'),  # latent layer (z)\n",
    "])\n",
    "\n",
    "\n",
    "# Decoder model\n",
    "decoder = tf.keras.Sequential([\n",
    "    tf.keras.Input(latent_dim),\n",
    "    tf.keras.layers.Dense(units=hidden_dim , activation='relu'),# hidden layer\n",
    "    tf.keras.layers.Dense(units=inp_dim , activation='sigmoid')# output layer\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "# Autoencoder model\n",
    "AE= tf.keras.models.Model( inputs = encoder.inputs, outputs =decoder(encoder.outputs), name= 'AE')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling and training the model\n",
    "* Hyperparameters\n",
    "* The error (cost) funstion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lr =  1e-4\n",
    "batch_size = 32\n",
    "epoch = 50\n",
    "\n",
    "\n",
    "AE.compile(optimizer=tf.keras.optimizers.Adam( lr ),              \n",
    "            loss =tf.keras.losses.binary_crossentropy)\n",
    "\n",
    "\n",
    "history_ae = AE.fit(x_train,\n",
    "                 x_train,\n",
    "                 batch_size = batch_size,\n",
    "                 epochs = epoch,\n",
    "                 validation_split = 0.3,\n",
    "                 shuffle= True,\n",
    "                 verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure('Training AE', figsize=(6,4))\n",
    "for k in history_ae.history.keys():\n",
    "    plt.plot(history_ae.history[k], label = k)\n",
    "plt.title('Model training')\n",
    "plt.ylabel('training error')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test data\n",
    "result = AE.evaluate(x_test, x_test, batch_size=batch_size, verbose= 0)\n",
    "print(\"Test loss:\", result)\n",
    "\n",
    "\n",
    "x_hat = AE(x_test).numpy()\n",
    "\n",
    "#index = np.random.choice(2000,  4)\n",
    "index = [  89, 1042,  989, 1866]\n",
    "fig, ax = plt.subplots(2, 4, num = 'Autoenocder reconstruction',\n",
    "                       subplot_kw={'xticks': [], 'yticks': []})\n",
    "\n",
    "for i, ind in enumerate(index):\n",
    "    ax[0,i].imshow(x_test[ind].reshape(28,28), cmap='gray')\n",
    "    ax[0,i].set_title(labeldict[y_test[ind]])\n",
    "    ax[1,i].imshow(x_hat[ind].reshape(28,28), cmap='gray')\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Variational autoencoder </h1> \n",
    " \n",
    " <br><br>\n",
    " <br><br>\n",
    " \n",
    " ### Tensoflow probability and probabilistic layers  [TFP](https://www.tensorflow.org/probability/api_docs/python/tfp/distributions)\n",
    " `!pip install --upgrade tensorflow-probability`\n",
    " \n",
    " * Latent distribution: We choose diagonal multivariate Gaussian (axis-aligned)<br><br>\n",
    "    $$q_\\phi(Z|X) =\\prod_{i} \\mathcal{N}(z_i| \\mu_i(X), \\sigma^2_i(X))$$  \n",
    "     $$z \\sim q(z|x)$$\n",
    "\n",
    " * Output layer: Bernoulli distribution<br><br>\n",
    " \n",
    " \n",
    "   `encoder = tf.keras.Sequential()` <br><br>\n",
    "   `encode.add(layer1), ...`<br><br>\n",
    "   `mu = Dense(latent_dim, 'linear')`<br><br>\n",
    "   `var = Dense(latent_dim, 'softplus')`<br><br>\n",
    "   `encoder.add(mu, var)`<br><br>\n",
    "   `q_z = tfp.distributions.MultivariateNormalDiag(loc = mu , scale_diag= var )`<br><br>   \n",
    "   \n",
    "   `decoder = tf.keras.Sequential()` <br><br>\n",
    "   `decode.add(tf.keras.layers.Dense(self.inp_shape, name='logits'))`<br><br>\n",
    "   `decode.add(tfp.layers.IndependentBernoulli(self.inp_shape, tfp.distributions.Bernoulli.logits)`<br><br>\n",
    "   `Autoenocder = tf.kera.Model(inputs = z, outputs = decoder(encoder.outputs)`\n",
    "   <br><br>\n",
    "   \n",
    " \n",
    "### VAE objective as log-likelihood of the distributions:\n",
    "Minimizing the evidence lower bound = negative log likelihood of data + KL divergence\n",
    "$$\\text{ELBO} \\approx -\\log p(x|z) + \\log q(z|x) -\\log p(z) $$\n",
    " \n",
    "Tensorflow probability provides log probability density function: `log_prob()` \n",
    "\n",
    "`p_z = tfp.distributions.MultivariateNormalDiag(latent_size)\n",
    "LL = p_z.log_prob(z)`\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(tf.keras.Model):\n",
    "    #subclassing the Model class\n",
    "    Default = {'encod_size':  None,\n",
    "               'decod_size': None,\n",
    "               'inp_shape':None,\n",
    "               'act_fun': 'relu', \n",
    "               'batch_size': 200,\n",
    "               'l2' : 0\n",
    "               }\n",
    "    \n",
    "    def __init__(self, hyper_p):\n",
    "        super(VAE, self).__init__()\n",
    "        self.__dict__.update(self.Default, **hyper_p )\n",
    "        \n",
    "        #prior\n",
    "        self.p_z = tfp.distributions.MultivariateNormalDiag(loc=tf.zeros(self.encod_size[-1]))\n",
    "        \n",
    "        self.encoder = self.make_encoder()\n",
    "        self.decoder = self.make_decoder()                  \n",
    "    \n",
    "    def posterior(self, args):\n",
    "        z_mu , z_var = args\n",
    "        return tfp.distributions.MultivariateNormalDiag(loc = z_mu , scale_diag= z_var ) \n",
    "            \n",
    "    def make_encoder(self):\n",
    "        inp = tf.keras.Input(self.inp_shape)\n",
    "        x = inp\n",
    "        \n",
    "        for lay in self.encod_size[:-1]:\n",
    "            x = tf.keras.layers.Dense(lay, \n",
    "                                       kernel_regularizer =tf.keras.regularizers.l2(self.l2),\n",
    "                                      activation = self.act_fun)(x)\n",
    "        z_mu = tf.keras.layers.Dense(self.encod_size[-1], activation = 'linear')(x)\n",
    "        z_var = tf.keras.layers.Dense(self.encod_size[-1], activation = 'softplus')(x)\n",
    "        \n",
    "        return tf.keras.models.Model(inp, [z_mu, z_var], name= 'Encoder')\n",
    "        \n",
    "        \n",
    "    def make_decoder(self):\n",
    "        inp = tf.keras.Input(self.encod_size[-1])\n",
    "        x = inp\n",
    "        for lay in self.decod_size:\n",
    "            \n",
    "            x = tf.keras.layers.Dense(lay,\n",
    "                                      kernel_regularizer =tf.keras.regularizers.l2(self.l2),\n",
    "                                      activation = self.act_fun)(x)\n",
    "            \n",
    "        # output =  tf.keras.layers.Dense(self.inp_shape,  activation = 'sigmoid', name='output')(x)   \n",
    "        logits = tf.keras.layers.Dense(self.inp_shape, name='output')(x)\n",
    "        output = tfp.layers.IndependentBernoulli(self.inp_shape, tfp.distributions.Bernoulli.logits)(logits)\n",
    "        \n",
    "        return tf.keras.models.Model(inp, output, name= 'Decoder')\n",
    "        \n",
    "    def call(self, inputs): \n",
    "        \n",
    "        z_mu, z_var = self.encoder(inputs)\n",
    "        q_z = self.posterior([z_mu, z_var])\n",
    "        z = q_z.sample()\n",
    "        \n",
    "        self.kl =  q_z.log_prob(z) - self.p_z.log_prob(z)\n",
    "        \n",
    "        reconstructed = self.decoder(z)\n",
    "        return reconstructed\n",
    "\n",
    "    \n",
    "    def total_loss(self, x, x_v):\n",
    "        \n",
    "        #ll = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=x_v, labels=x))\n",
    "        ll = - x_v.log_prob(x)\n",
    "        return tf.reduce_mean( ll + self.kl)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "hyp_param ={ 'inp_shape': x_train.shape[1],\n",
    "             'encod_size' : [50, 3],\n",
    "             'decod_size':  [ 50],\n",
    "             'l2':1e-2,\n",
    "             'act_fun': 'relu',\n",
    "             'batch_size' : 32,\n",
    "             'epochs' : 50,\n",
    "             'learning_rate' : 1e-4\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "vae = VAE(hyp_param)\n",
    "\n",
    "vae.compile(optimizer=tf.keras.optimizers.Adam(lr=hyp_param['learning_rate']),              \n",
    "            loss = vae.total_loss)\n",
    "\n",
    "\n",
    "history_vae = vae.fit(x_train,\n",
    "                 x_train,\n",
    "                 batch_size=hyp_param['batch_size'],\n",
    "                 epochs=hyp_param['epochs'],\n",
    "                 validation_split = 0.3,\n",
    "                 shuffle= True,\n",
    "                 verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_hat = vae(x_test).mean().numpy()\n",
    "vae.encoder.summary()\n",
    "\n",
    "\n",
    "index = np.random.choice(2000,  5)\n",
    "\n",
    "fig, ax = plt.subplots(2, 5, num = 'Variational autoenocder reconstruction',\n",
    "                       subplot_kw={'xticks': [], 'yticks': []})\n",
    "\n",
    "for i, ind in enumerate(index):\n",
    "    ax[0,i].imshow(x_test[ind].reshape(28,28), cmap='gray')\n",
    "    ax[0,i].set_title(labeldict[y_test[ind]])\n",
    "    ax[1,i].imshow(x_hat[ind].reshape(28,28), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
